Скринкаст (запись экрана) доступен по данной ссылке https://drive.google.com/drive/folders/1b2Xq7huVhoxWQeeobrxgWxU6rgZAVdDi?usp=sharing

# Курсовая работа  
## Параллельное программирование на суперкомпьютерных системах  
**Вариант 25 — Быстрое преобразование Фурье (FFT)**

**Студент:** Черепанов Н.И.  
**Группа:** 5140201/50301  
**Руководитель:** Лукашин А.А  
**СПбПУ Петра Великого, ИКНК, ВШТИИ**  
**2025**

---

## Постановка задачи

Цель работы — исследовать эффективность различных подходов к распараллеливанию вычислений на примере алгоритма быстрого преобразования Фурье (FFT).

В рамках работы необходимо:

- изучить алгоритм FFT и способы его распараллеливания;
- подготовить тесты для проверки корректности результатов;
- реализовать FFT четырьмя способами:
  - **C + pthreads**
  - **C + OpenMP**
  - **C + MPI**
  - **Python + MPI (mpi4py)**
- провести вычислительные эксперименты на суперкомпьютере, варьируя:
  - до **48 потоков** (pthreads, OpenMP),
  - до **112 MPI-процессов**;
- исследовать эффективность распараллеливания (время, ускорение);
- подготовить отчет и демонстрацию запуска программ.

---

## Алгоритм FFT и возможности распараллеливания

FFT (Fast Fourier Transform) — алгоритм вычисления дискретного преобразования Фурье со сложностью  
**O(N log N)**, основанный на рекурсивном разбиении исходной последовательности на чётные и нечётные элементы.

### Возможности параллелизации

- параллельное выполнение «бабочек» (butterfly operations);
- независимость вычислений внутри одного уровня FFT;
- естественная поддержка shared-memory параллелизма.

### Ограничения

- алгоритм ограничен **пропускной способностью памяти**;
- требуется синхронизация потоков/процессов;
- MPI-реализация на **одном узле** часто неэффективна из-за накладных расходов обмена.

---

## Проверка корректности

Для проверки корректности результатов использовались:

- детерминированный тестовый сигнал;
- сравнение норм результата:
norm2 = Σ (Re(x[i])² + Im(x[i])²)


- допустимая погрешность вычислений — **1e-6**.

Все реализации (C serial, pthreads, OpenMP, MPI, Python MPI) выдают совпадающие результаты в пределах ошибок округления, что подтверждает корректность вычислений.

---

## Реализация алгоритмов

### 1. C + pthreads

Распараллеливание реализовано вручную с использованием `pthread_create` и `pthread_join`.

**Плюсы:**
- контроль над распределением задач;
- хорошая производительность.

**Минусы:**
- сложность реализации;
- ручное управление синхронизацией.

---

### 2. C + OpenMP

Используются директивы `#pragma omp parallel for`.

**Плюсы:**
- простота реализации;
- лучшая производительность среди всех подходов;
- эффективное использование shared-memory архитектуры.

**Минусы:**
- ограниченная масштабируемость при большом числе потоков.

---

### 3. C + MPI

Данные распределяются между процессами, для глобальных стадий FFT используется `MPI_Gather / MPI_Scatter`.

**Минусы:**
- большие накладные расходы обмена;
- неэффективен на одном узле;
- ухудшение производительности при росте числа процессов.

---

### 4. Python + MPI (mpi4py)

Реализация аналогична C MPI, но с использованием NumPy и mpi4py.

**Особенности:**
- дополнительный overhead Python;
- сериализация данных;
- медленное взаимодействие NumPy и MPI.

---

## Экспериментальная установка

- Суперкомпьютер СПбПУ, кластер **tornado**
- Один вычислительный узел (48 CPU потоков)
- OpenMPI 4.1.6
- Intel MPI 2018
- Python Intel 3.6.3 + mpi4py
- Размер задачи:  
N = 1 048 576

- Каждый эксперимент выполнялся **3 раза**, результаты усреднялись

---

## Результаты экспериментов

CSV-файл с результатами:  
`bench_results_20251208_213557.csv`

---

## Описание графиков

### Рисунок 1 — Время выполнения FFT в зависимости от числа потоков/процессов

На графике показано изменение времени выполнения алгоритма FFT при использовании различных моделей параллелизма.

- **OpenMP** — самая быстрая реализация. Минимальное время достигается при 8–16 потоках (~0.09 сек).
- **pthreads** — поведение близко к OpenMP, но немного хуже.
- **MPI (C)** — при увеличении числа процессов время выполнения возрастает.
- **Python MPI** — самая медленная реализация.
- **Последовательная версия** служит базовой линией сравнения.

---

### Рисунок 2 — Ускорение относительно последовательной реализации

График показывает ускорение (speedup), рассчитанное как отношение времени последовательной версии ко времени параллельной.

- **OpenMP** достигает максимального ускорения ~1.5×.
- **pthreads** — до ~1.2×.
- **MPI (C)** демонстрирует отрицательное масштабирование.
- **Python MPI** ускоряется, но стартует с большого overhead.

---

### Рисунок 3 — Сравнительный график всех реализаций

На одном графике представлены все реализации FFT.

- OpenMP уверенно лидирует.
- pthreads близок по эффективности.
- MPI значительно проигрывает на одном узле.
- Python MPI остаётся самым медленным вариантом.

---

## Почему не везде удалось использовать 48 потоков

- **MPI** использует процессы, а не потоки — масштабирование ограничено коммуникациями.
- **FFT** ограничена памятью, а не вычислениями.
- После 8–16 потоков рост накладных расходов превышает выигрыш от параллелизма.
- Использование 48 потоков приводит к насыщению памяти и падению эффективности.

---

## Итоговые выводы

- Лучший способ распараллеливания FFT на одном узле — **OpenMP**.
- FFT плохо масштабируется из-за memory-bound природы алгоритма.
- MPI эффективен только на **распределённых узлах**, а не внутри одного.
- Python MPI работает корректно, но значительно уступает C-реализациям.
- Полученные результаты соответствуют теоретическим ожиданиям.

---

## Команды запуска (для демонстрации)

```bash
# Сборка
make clean
make -j

# Последовательная версия
./fft_serial 1048576

# pthreads
./fft_pthreads 1048576 8

# OpenMP
export OMP_NUM_THREADS=8
./fft_openmp 1048576

# MPI (C)
mpirun -np 4 ./fft_mpi 1048576

# MPI (Python)
mpirun -n 4 python3 fft_mpi.py 1048576
```
Приложение

Исходные коды:

fft_serial.c

fft_pthreads.c

fft_openmp.c

fft_mpi.c

fft_mpi.py

Санкт-Петербург, 2025
